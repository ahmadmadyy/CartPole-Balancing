{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8a17ad-7280-4ec6-a308-95beb9f35648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Khaled\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:177: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "\n",
    "def load_pretrained_model(path):\n",
    "    \"\"\"Load a pretrained model from the path provided as parameter\"\"\"\n",
    "    return keras.models.load_model(path)\n",
    "\n",
    "\n",
    "def select_trained_agent_action(state, trained_model):\n",
    "    \"\"\"Uses the trained model to predict the action with highest Q-Value\"\"\"\n",
    "    q_values = trained_model.predict(state, verbose=0)\n",
    "    return np.argmax(q_values[0])\n",
    "\n",
    "\n",
    "def plot_epsilon_values():\n",
    "    \"\"\"Plots the different values for Epsilon during the training\"\"\"\n",
    "    with open('epsilon_values.txt', 'r') as f:\n",
    "        file = f.read()\n",
    "        epsilon_values = json.loads(file)\n",
    "\n",
    "    plt.plot(range(len(epsilon_values)), epsilon_values)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_rewards(calculate_mean=1):\n",
    "    \"\"\"Plots the rewards obtained during the training. The total number of episodes must be\n",
    "    a multiple of 'calculate_mean' parameter\"\"\"\n",
    "    with open('rewards.txt', 'r') as f:\n",
    "        file = f.read()\n",
    "        rewards = json.loads(file)\n",
    "\n",
    "    if len(rewards) % calculate_mean != 0:\n",
    "        raise ValueError\n",
    "\n",
    "    # Calculate mean of rewards\n",
    "    mean_rewards = list()\n",
    "    for i in range(round(len(rewards)/calculate_mean)):\n",
    "        reward_range = rewards[i*calculate_mean:i*calculate_mean+calculate_mean]\n",
    "        reward_mean = round(sum(reward_range)/len(reward_range))\n",
    "        mean_rewards.append(reward_mean)\n",
    "\n",
    "    plt.plot(range(len(mean_rewards)), mean_rewards)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    env = gym.make('CartPole-v1', render_mode='human')\n",
    "    state, _ = env.reset()\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    terminal = False\n",
    "    trained_agent = load_pretrained_model('trained_agent.h5')\n",
    "\n",
    "    total_reward = 0\n",
    "    max_timesteps = 500\n",
    "\n",
    "    # Execute Episode\n",
    "    for t in range(max_timesteps):\n",
    "\n",
    "        env.render()\n",
    "        state = state.reshape((1, state_size))\n",
    "        action = select_trained_agent_action(state, trained_agent)\n",
    "        next_state, reward, terminal, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "    plot_rewards(1)\n",
    "    plot_epsilon_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723ad2de-1a84-4431-809a-57d31008c40f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
